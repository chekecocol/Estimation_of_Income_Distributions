{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as sp_opt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of data:\n",
      "              0    1\n",
      "0  14609.870117  915\n",
      "1  22263.830078  915\n",
      "2   2393.419922  915\n",
      "3  18200.000000  915\n",
      "4   5983.560059  915\n",
      "5  12465.750000  915\n",
      "6  13213.700195  968\n",
      "7  12553.009766  968\n",
      "8     24.930000  968\n",
      "9  41884.921875  968\n",
      "\n",
      "First 10 elements of Y tensor:\n",
      "[4.86995649 7.42127657 0.797806621 ... 4.40456676 4.18433666 0.00831]\n",
      "\n",
      "First 10 elements of Weights tensor:\n",
      "[915 915 915 ... 968 968 968]\n",
      "\n",
      "Expanded number of observations in data:\n",
      "73871328\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "data = pd.read_excel(\"~/Desktop/datos_enigh.xls\", header = None)\n",
    "print(\"\\nFirst 10 rows of data:\")\n",
    "print(data.head(10))\n",
    "data = data.to_numpy()\n",
    "# Transform to monthly income in thousands\n",
    "data = data / np.array([3000,1])\n",
    "\n",
    "# Cast into Tensorflow objects\n",
    "Y = tf.constant(data[:,0], dtype = tf.float32)\n",
    "Weights = tf.constant(data[:,1], dtype = tf.float32)\n",
    "expanded_n = tf.math.reduce_sum(Weights)\n",
    "\n",
    "# Print Tensorflow objects\n",
    "print(\"\\nFirst 10 elements of Y tensor:\")\n",
    "tf.print(Y[0:9])\n",
    "print(\"\\nFirst 10 elements of Weights tensor:\")\n",
    "tf.print(Weights[0:9])\n",
    "print(\"\\nExpanded number of observations in data:\")\n",
    "tf.print(expanded_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=float32, numpy=198604540.0>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([  1282176. ,  -2394799.8, -21291136. ], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "## Test likelihood function of the Generalized Gamma distribution in Tensorflow\n",
    "test_param = tf.constant([3, 1., 0.7], dtype = tf.float32)\n",
    "\n",
    "# Define target negative log-likelihood function without constraints\n",
    "@tf.function\n",
    "def target_GeneralizedGamma(param_vec, Y = Y, Weights = Weights, n = expanded_n):\n",
    "    # Retrieve distribution parameters\n",
    "    a, d, p = tf.split(param_vec, 3, axis = 0)\n",
    "    \n",
    "    # Normalize weights\n",
    "    #weights = weights / n\n",
    "    #tf.print(weights[0:9])\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    ll = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p))\n",
    "    \n",
    "    return tf.squeeze(ll)\n",
    "\n",
    "# Return target function and its gradient\n",
    "@tf.function\n",
    "def target_grad_GeneralizedGamma(param_vec):\n",
    "    return tfp.math.value_and_gradient(target_GeneralizedGamma, param_vec)\n",
    "\n",
    "print( target_grad_GeneralizedGamma(test_param) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of optimizer:\n",
      "BfgsOptimizerResults(converged=1, failed=0, num_iterations=9, num_objective_evaluations=69, position=[2.88551736 1.144014 0.772850096], objective_value=197576048, objective_gradient=[-37150 -133019.375 289104], inverse_hessian_estimate=[[0.988262296 -0.0351199284 0.10210707]\n",
      " [-0.0351199284 0.899715602 0.312423259]\n",
      " [0.10210707 0.312423259 0.121726513]])\n",
      "\n",
      "Estimated parameters:\n",
      "[2.8855174 1.144014  0.7728501]\n",
      "\n",
      "Mean of fitted distribution:\n",
      "5.3449636313359\n"
     ]
    }
   ],
   "source": [
    "## Minimize negative log-likelihood using BFGS in Tensorflow\n",
    "start_param = test_param\n",
    "\n",
    "optim_results = tfp.optimizer.bfgs_minimize(target_grad_GeneralizedGamma, start_param, tolerance = 1e-10)\n",
    "print(\"\\nOutput of optimizer:\")\n",
    "tf.print(optim_results)\n",
    "est_params = optim_results.position.numpy()\n",
    "\n",
    "a_fitted = tf.constant(est_params[0], dtype = tf.float64)\n",
    "d_fitted = tf.constant(est_params[1], dtype = tf.float64)\n",
    "p_fitted = tf.constant(est_params[2], dtype = tf.float64)\n",
    "\n",
    "print(\"\\nEstimated parameters:\")\n",
    "print(est_params)\n",
    "\n",
    "# Compute fitted mean\n",
    "mu_fitted = a_fitted * tf.math.exp(tf.math.lgamma((d_fitted+1)/p_fitted)) / tf.math.exp(tf.math.lgamma(d_fitted/p_fitted))\n",
    "print(\"\\nMean of fitted distribution:\")\n",
    "tf.print(mu_fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient at iteration 0\n",
      "[-67603888, -82420384, 274317472, 0.815295696]\n",
      "Parameter values at iteration 0\n",
      "2\n",
      "0.92\n",
      "1.01\n",
      "100000\n",
      "\n",
      "Gradient at iteration 10000\n",
      "[nan, nan, nan, nan]\n",
      "Parameter values at iteration 10000\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "Gradient Descent took 24.034006118774414 seconds\n",
      "\n",
      "Mean of fitted distribution:\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "## Minimize negative log-likelihood using gradient descent\n",
    "@tf.function\n",
    "def SGDfunction(a, d, p, lambda1, Y = Y, Weights = Weights, n = expanded_n):\n",
    "    target = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p)) \\\n",
    "        + lambda1 * ((a * tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p))) - 1)\n",
    "    \n",
    "    return tf.squeeze(target)\n",
    "\n",
    "@tf.function\n",
    "def SGDfunction_minimize(Y = Y, Weights = Weights, n = expanded_n):\n",
    "    target = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p)) \\\n",
    "        + lambda1 * ((a * tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p))) - 1)\n",
    "    \n",
    "    return tf.squeeze(target)\n",
    "\n",
    "# Generate tensors with initial values\n",
    "def SGD_initial_values():\n",
    "    a = tf.Variable(2.00) \n",
    "    d = tf.Variable(0.92)\n",
    "    p = tf.Variable(1.01) \n",
    "    lambda1 = tf.Variable(1e5*1.0) \n",
    "    return a, d, p, lambda1\n",
    "\n",
    "a, d, p, lambda1 = SGD_initial_values()\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = 3e-9)\n",
    "tic = time.time()\n",
    "for i in range(10001):\n",
    "    if i%10000 == 0:\n",
    "        print(\"\\nGradient at iteration \" + str(i))\n",
    "        with tf.GradientTape() as tape:\n",
    "            function = SGDfunction(a, d, p, lambda1)\n",
    "        tf.print(tape.gradient(function, [a, d, p, lambda1]))\n",
    "        print(\"Parameter values at iteration \" + str(i))\n",
    "        tf.print(a)\n",
    "        tf.print(d)\n",
    "        tf.print(p)\n",
    "        tf.print(lambda1)\n",
    "    opt.minimize(SGDfunction_minimize, var_list = [a, d, p, lambda1])\n",
    "print(\"\\nGradient Descent took \"+str(time.time()-tic)+\" seconds\")\n",
    "\n",
    "# Compute fitted mean\n",
    "mu_fitted_2 = a * tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p))\n",
    "print(\"\\nMean of fitted distribution:\")\n",
    "tf.print(mu_fitted_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of optimizer:\n",
      "BfgsOptimizerResults(converged=1, failed=0, num_iterations=11, num_objective_evaluations=66, position=[1.75941372 0.285252124], objective_value=204857120, objective_gradient=[104 1152], inverse_hessian_estimate=[[2.23848156e-06 -4.07764787e-07]\n",
      " [-4.07764787e-07 7.66302435e-08]])\n",
      "\n",
      "Estimated parameters:\n",
      "0.00924701244\n",
      "1.75941372\n",
      "0.285252124\n",
      "\n",
      "Constraint is satisfied. Mean equals:\n",
      "10.1\n",
      "\n",
      "Density above threshold of 200.0:\n",
      "0.000691972731\n"
     ]
    }
   ],
   "source": [
    "## Perform Maximum Likelihood Estimation with constraints using BFGS in Tensorflow\n",
    "start_param_constrained = tf.constant([1.5, .5], dtype = tf.float32)\n",
    "\n",
    "constrained_mean = 10.1\n",
    "top_threshold = 200.\n",
    "constrained_density = 0.0005\n",
    "\n",
    "# Define target negative log-likelihood function with constraints\n",
    "@tf.function\n",
    "def target_GeneralizedGamma_Constrained(param_vec_constrained, Y = Y, Weights = Weights, n = expanded_n):\n",
    "    # Retrieve distribution parameters\n",
    "    d, p = tf.split(param_vec_constrained, 2, axis = 0)\n",
    "    \n",
    "    # Constraint over the mean of the distribution\n",
    "    a = constrained_mean / (tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p)))\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    ll = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p))\n",
    "    \n",
    "    return tf.squeeze(ll)\n",
    "\n",
    "# Return target function and its gradient\n",
    "@tf.function\n",
    "def target_grad_GeneralizedGamma_Constrained(param_vec_constrained):\n",
    "    return tfp.math.value_and_gradient(target_GeneralizedGamma_Constrained, param_vec_constrained)\n",
    "\n",
    "# Minimize negative log-likelihood via BFGS\n",
    "optim_results_constrained = tfp.optimizer.bfgs_minimize(target_grad_GeneralizedGamma_Constrained, start_param_constrained, tolerance = 1e-10)\n",
    "print(\"\\nOutput of optimizer:\")\n",
    "tf.print(optim_results_constrained)\n",
    "est_params_constrained = optim_results_constrained.position.numpy()\n",
    "\n",
    "d_fitted_constrained = tf.constant(est_params_constrained[0], dtype = tf.float32)\n",
    "p_fitted_constrained = tf.constant(est_params_constrained[1], dtype = tf.float32)\n",
    "a_fitted_constrained = constrained_mean / (tf.math.exp(tf.math.lgamma((d_fitted_constrained+1)/p_fitted_constrained)) / tf.math.exp(tf.math.lgamma(d_fitted_constrained/p_fitted_constrained)))\n",
    "\n",
    "print(\"\\nEstimated parameters:\")\n",
    "tf.print(a_fitted_constrained)\n",
    "tf.print(d_fitted_constrained)\n",
    "tf.print(p_fitted_constrained)\n",
    "\n",
    "print(\"\\nConstraint is satisfied. Mean equals:\")\n",
    "mu_fitted_constrained = a_fitted_constrained * tf.math.exp(tf.math.lgamma((d_fitted_constrained+1)/p_fitted_constrained)) / tf.math.exp(tf.math.lgamma(d_fitted_constrained/p_fitted_constrained))\n",
    "tf.print(mu_fitted_constrained)\n",
    "\n",
    "# Verify second constraint\n",
    "@tf.function\n",
    "def CDF_GeneralizedGamma(x, a, p, d):\n",
    "    # Using the inverse gamma distribution this way yields the same as the CDF for the generalized gamma distribution\n",
    "    return tf.math.igammac( (d/p), ((x/a)**p) )\n",
    "\n",
    "# Density above threshold:\n",
    "print(\"\\nDensity above threshold of \" + str(top_threshold) +\":\")\n",
    "tf.print( CDF_GeneralizedGamma(top_threshold, a_fitted_constrained, p_fitted_constrained, d_fitted_constrained) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 197575964.159925\n",
      "         Iterations: 14\n",
      "         Function evaluations: 312\n",
      "         Gradient evaluations: 60\n",
      "\n",
      "Unconstrained solution:\n",
      "[2.89119922 1.14418363 0.77303309]\n",
      "\n",
      "Unconstrained mean of the fitted distribution:\n",
      "5.353559091838916\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 204857109.72523957\n",
      "            Iterations: 15\n",
      "            Function evaluations: 168\n",
      "            Gradient evaluations: 15\n",
      "\n",
      "Constrained solution:\n",
      "[0.00924035 1.75949774 0.28523676]\n",
      "\n",
      "Constrained mean of the fitted distribution:\n",
      "10.09999999999812\n",
      "\n",
      "Density above threshold of 200.0:\n",
      "0.0006920395570788554\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 204886625.7480954\n",
      "            Iterations: 92\n",
      "            Function evaluations: 599\n",
      "            Gradient evaluations: 92\n",
      "\n",
      "Constrained solution:\n",
      "[0.04673874 1.58409512 0.32726759]\n",
      "\n",
      "Constrained mean of the fitted distribution:\n",
      "10.099999999983272\n",
      "\n",
      "Density above threshold of 200.0:\n",
      "0.00049999999999386\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Perform Maximum Likelihood Estimation with constraints using BFGS in Scipy\n",
    "\n",
    "start_param_scipy = test_param.numpy()\n",
    "\n",
    "mydata = [data[:,0], data[:,1], np.sum(data[:,1])]\n",
    "\n",
    "def objective_GeneralizedGamma(params, mydata):\n",
    "    # Retrieve parameters\n",
    "    a = params[0]\n",
    "    d = params[1]\n",
    "    p = params[2]\n",
    "    \n",
    "    # Retrieve observations\n",
    "    Y = mydata[0]\n",
    "    Weights = mydata[1]\n",
    "    n = mydata[2]\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    ll = -(n*np.log((p/a**d)/sp.special.gamma(d/p)) + (d-1)*np.sum(Weights*np.log(Y)) - np.sum(Weights*(Y/a)**p))\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def Mean_GeneralizedGamma(params):\n",
    "    a = params[0]\n",
    "    d = params[1]\n",
    "    p = params[2]\n",
    "    \n",
    "    mean = a * sp.special.gamma((d+1)/p) / sp.special.gamma(d/p)\n",
    "    return mean\n",
    "\n",
    "def constrained_mean_fun(params):\n",
    "    cons_mean = Mean_GeneralizedGamma(params) - constrained_mean\n",
    "    return cons_mean\n",
    "\n",
    "def Right_CDF_GeneralizedGamma(params):\n",
    "    a = params[0]\n",
    "    d = params[1]\n",
    "    p = params[2]\n",
    "    \n",
    "    right_cdf = sp.special.gammaincc(d/p, (top_threshold/a)**p)\n",
    "    return right_cdf\n",
    "\n",
    "def constrained_CDF_fun(params):\n",
    "    cons_CDF = Right_CDF_GeneralizedGamma(params) - constrained_density\n",
    "    return cons_CDF\n",
    "\n",
    "# Firstly, test unconstrained optimization\n",
    "#print( objective_GeneralizedGamma(start_param_scipy, mydata) )\n",
    "myoptions = {'disp' : True, 'maxiter' : 1000}\n",
    "unc_results = sp_opt.minimize(objective_GeneralizedGamma, start_param_scipy, args = mydata, options = myoptions)\n",
    "print(\"\\nUnconstrained solution:\")\n",
    "print(unc_results.x)\n",
    "print(\"\\nUnconstrained mean of the fitted distribution:\")\n",
    "unc_fitted_mean = Mean_GeneralizedGamma(unc_results.x)\n",
    "print(unc_fitted_mean)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Then, perform optimization over the constrained mean\n",
    "con_start_param_scipy = [a_fitted_constrained, d_fitted_constrained, p_fitted_constrained]\n",
    "myconstraints = ({'type': 'eq', 'fun' : constrained_mean_fun})\n",
    "con_results = sp_opt.minimize(objective_GeneralizedGamma, con_start_param_scipy, args = mydata, constraints = myconstraints, options = myoptions)\n",
    "print(\"\\nConstrained solution:\")\n",
    "print(con_results.x)\n",
    "print(\"\\nConstrained mean of the fitted distribution:\")\n",
    "con_fitted_mean = Mean_GeneralizedGamma(con_results.x)\n",
    "print(con_fitted_mean)\n",
    "print(\"\\nDensity above threshold of \" + str(top_threshold) +\":\")\n",
    "print(Right_CDF_GeneralizedGamma(con_results.x))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Finally, perform optimization over the constrained mean and the constrained cdf\n",
    "con_start_param_scipy = [a_fitted_constrained, d_fitted_constrained, p_fitted_constrained]\n",
    "myconstraints = ({'type': 'eq', 'fun' : constrained_mean_fun}, {'type': 'eq', 'fun' : constrained_CDF_fun})\n",
    "con_results = sp_opt.minimize(objective_GeneralizedGamma, con_start_param_scipy, args = mydata, constraints = myconstraints, options = myoptions)\n",
    "print(\"\\nConstrained solution:\")\n",
    "print(con_results.x)\n",
    "print(\"\\nConstrained mean of the fitted distribution:\")\n",
    "con_fitted_mean = Mean_GeneralizedGamma(con_results.x)\n",
    "print(con_fitted_mean)\n",
    "print(\"\\nDensity above threshold of \" + str(top_threshold) +\":\")\n",
    "print(Right_CDF_GeneralizedGamma(con_results.x))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
