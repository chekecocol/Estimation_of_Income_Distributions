{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as sp_opt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of data:\n",
      "              0     1\n",
      "0   5869.560059  1537\n",
      "1  25679.339844  1537\n",
      "2  21277.160156  1537\n",
      "3   7092.379883  1537\n",
      "4  54293.468750  1537\n",
      "5  30091.289062  1537\n",
      "6  32624.990234  1537\n",
      "7  27880.429688  1537\n",
      "8  26657.599609  1537\n",
      "9   5282.600098  1215\n",
      "\n",
      "First 10 elements of Y tensor:\n",
      "[1.95652 8.55978 7.09238672 ... 10.8749971 9.2934761 8.88586617]\n",
      "\n",
      "First 10 elements of Weights tensor:\n",
      "[1537 1537 1537 ... 1537 1537 1537]\n",
      "\n",
      "Expanded number of observations in data:\n",
      "38294416\n",
      "\n",
      "Average income in data:\n",
      "5.43961477\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "data = pd.read_excel(\"~/Desktop/enigh_income_data_labor.xls\", header = None)\n",
    "print(\"\\nFirst 10 rows of data:\")\n",
    "print(data.head(10))\n",
    "data = data.to_numpy()\n",
    "# Transform to monthly income in thousands\n",
    "data = data / np.array([3000,1])\n",
    "\n",
    "# Cast into Tensorflow objects\n",
    "Y = tf.constant(data[:,0], dtype = tf.float32)\n",
    "Weights = tf.constant(data[:,1], dtype = tf.float32)\n",
    "expanded_n = tf.math.reduce_sum(Weights)\n",
    "\n",
    "# Print Tensorflow objects\n",
    "print(\"\\nFirst 10 elements of Y tensor:\")\n",
    "tf.print(Y[0:9])\n",
    "print(\"\\nFirst 10 elements of Weights tensor:\")\n",
    "tf.print(Weights[0:9])\n",
    "print(\"\\nExpanded number of observations in data:\")\n",
    "tf.print(expanded_n)\n",
    "print(\"\\nAverage income in data:\")\n",
    "tf.print(tf.reduce_sum(Weights*Y)/expanded_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=float32, numpy=105164056.0>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-3445576., -9397222., 25728128.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "## Test likelihood function of the Generalized Gamma distribution in Tensorflow\n",
    "test_param = tf.constant([4., 1.0, 1.0], dtype = tf.float32)\n",
    "\n",
    "# Define target negative log-likelihood function without constraints\n",
    "@tf.function\n",
    "def target_GeneralizedGamma(param_vec, Y = Y, Weights = Weights, n = expanded_n):\n",
    "    # Retrieve distribution parameters\n",
    "    a, d, p = tf.split(param_vec, 3, axis = 0)\n",
    "    \n",
    "    # Normalize weights\n",
    "    #weights = weights / n\n",
    "    #tf.print(weights[0:9])\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    ll = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p))\n",
    "    \n",
    "    return tf.squeeze(ll)\n",
    "\n",
    "# Return target function and its gradient\n",
    "@tf.function\n",
    "def target_grad_GeneralizedGamma(param_vec):\n",
    "    return tfp.math.value_and_gradient(target_GeneralizedGamma, param_vec)\n",
    "\n",
    "print( target_grad_GeneralizedGamma(test_param) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of optimizer:\n",
      "BfgsOptimizerResults(converged=1, failed=0, num_iterations=11, num_objective_evaluations=47, position=[3.85739279 1.02209401 0.82695514], objective_value=102917600, objective_gradient=[2 25 -28], inverse_hessian_estimate=[[0.000245528645 -1.82728709e-05 1.86315447e-05]\n",
      " [-1.82728709e-05 1.37505299e-06 -1.37900315e-06]\n",
      " [1.86315447e-05 -1.37900315e-06 1.42219676e-06]])\n",
      "\n",
      "Estimated parameters:\n",
      "[3.8573928  1.022094   0.82695514]\n",
      "\n",
      "Mean of fitted distribution:\n",
      "5.42960381096186\n"
     ]
    }
   ],
   "source": [
    "## Minimize negative log-likelihood using BFGS in Tensorflow\n",
    "start_param = test_param\n",
    "\n",
    "optim_results = tfp.optimizer.bfgs_minimize(target_grad_GeneralizedGamma, start_param, tolerance = 1e-10)\n",
    "print(\"\\nOutput of optimizer:\")\n",
    "tf.print(optim_results)\n",
    "est_params = optim_results.position.numpy()\n",
    "\n",
    "a_fitted = tf.constant(est_params[0], dtype = tf.float64)\n",
    "d_fitted = tf.constant(est_params[1], dtype = tf.float64)\n",
    "p_fitted = tf.constant(est_params[2], dtype = tf.float64)\n",
    "\n",
    "print(\"\\nEstimated parameters:\")\n",
    "print(est_params)\n",
    "\n",
    "# Compute fitted mean\n",
    "mu_fitted = a_fitted * tf.math.exp(tf.math.lgamma((d_fitted+1)/p_fitted)) / tf.math.exp(tf.math.lgamma(d_fitted/p_fitted))\n",
    "print(\"\\nMean of fitted distribution:\")\n",
    "tf.print(mu_fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient at iteration 0\n",
      "[-3345576, -8997222, 25159012, 3]\n",
      "Parameter values at iteration 0\n",
      "4\n",
      "1\n",
      "1\n",
      "100000\n",
      "\n",
      "Gradient at iteration 10000\n",
      "[2189, -161.421875, 192, 4.34211063]\n",
      "Parameter values at iteration 10000\n",
      "3.83816457\n",
      "1.02017117\n",
      "0.830833733\n",
      "100000\n",
      "\n",
      "Gradient Descent took 20.938907861709595 seconds\n",
      "\n",
      "Mean of fitted distribution:\n",
      "5.34211159\n"
     ]
    }
   ],
   "source": [
    "## Minimize negative log-likelihood using gradient descent\n",
    "@tf.function\n",
    "def SGDfunction(a, d, p, lambda1, Y = Y, Weights = Weights, n = expanded_n):\n",
    "    target = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p)) \\\n",
    "        + lambda1 * ((a * tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p))) - 1)\n",
    "    \n",
    "    return tf.squeeze(target)\n",
    "\n",
    "@tf.function\n",
    "def SGDfunction_minimize(Y = Y, Weights = Weights, n = expanded_n):\n",
    "    target = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p)) \\\n",
    "        + lambda1 * ((a * tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p))) - 1)\n",
    "    \n",
    "    return tf.squeeze(target)\n",
    "\n",
    "# Generate tensors with initial values\n",
    "def SGD_initial_values():\n",
    "    a = tf.Variable(4.00) \n",
    "    d = tf.Variable(1.00)\n",
    "    p = tf.Variable(1.00) \n",
    "    lambda1 = tf.Variable(1e5*1.0) \n",
    "    return a, d, p, lambda1\n",
    "\n",
    "a, d, p, lambda1 = SGD_initial_values()\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = 3e-9)\n",
    "tic = time.time()\n",
    "for i in range(10001):\n",
    "    if i%10000 == 0:\n",
    "        print(\"\\nGradient at iteration \" + str(i))\n",
    "        with tf.GradientTape() as tape:\n",
    "            function = SGDfunction(a, d, p, lambda1)\n",
    "        tf.print(tape.gradient(function, [a, d, p, lambda1]))\n",
    "        print(\"Parameter values at iteration \" + str(i))\n",
    "        tf.print(a)\n",
    "        tf.print(d)\n",
    "        tf.print(p)\n",
    "        tf.print(lambda1)\n",
    "    opt.minimize(SGDfunction_minimize, var_list = [a, d, p, lambda1])\n",
    "print(\"\\nGradient Descent took \"+str(time.time()-tic)+\" seconds\")\n",
    "\n",
    "# Compute fitted mean\n",
    "mu_fitted_2 = a * tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p))\n",
    "print(\"\\nMean of fitted distribution:\")\n",
    "tf.print(mu_fitted_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of optimizer:\n",
      "BfgsOptimizerResults(converged=1, failed=0, num_iterations=7, num_objective_evaluations=45, position=[1.09829903 0.588815212], objective_value=104750208, objective_gradient=[-478 -708], inverse_hessian_estimate=[[1.95692238e-07 -1.28930125e-07]\n",
      " [-1.28930125e-07 1.14102107e-07]])\n",
      "\n",
      "Estimated parameters:\n",
      "2.13125801\n",
      "1.09829903\n",
      "0.588815212\n",
      "\n",
      "Constraint is satisfied. Mean equals:\n",
      "8\n",
      "\n",
      "Density above threshold of 200.0:\n",
      "5.68335781e-06\n"
     ]
    }
   ],
   "source": [
    "## Perform Maximum Likelihood Estimation with constraints using BFGS in Tensorflow\n",
    "start_param_constrained = tf.constant([1.5, .5], dtype = tf.float32)\n",
    "\n",
    "constrained_mean = 8.\n",
    "top_threshold = 200.\n",
    "constrained_density = 0.0005\n",
    "\n",
    "# Define target negative log-likelihood function with constraints\n",
    "@tf.function\n",
    "def target_GeneralizedGamma_Constrained(param_vec_constrained, Y = Y, Weights = Weights, n = expanded_n):\n",
    "    # Retrieve distribution parameters\n",
    "    d, p = tf.split(param_vec_constrained, 2, axis = 0)\n",
    "    \n",
    "    # Constraint over the mean of the distribution\n",
    "    a = constrained_mean / (tf.math.exp(tf.math.lgamma((d+1)/p)) / tf.math.exp(tf.math.lgamma(d/p)))\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    ll = -(n*tf.math.log((p/a**d)/tf.math.exp(tf.math.lgamma(d/p))) + (d-1)*tf.math.reduce_sum(Weights*tf.math.log(Y)) - tf.math.reduce_sum(Weights*(Y/a)**p))\n",
    "    \n",
    "    return tf.squeeze(ll)\n",
    "\n",
    "# Return target function and its gradient\n",
    "@tf.function\n",
    "def target_grad_GeneralizedGamma_Constrained(param_vec_constrained):\n",
    "    return tfp.math.value_and_gradient(target_GeneralizedGamma_Constrained, param_vec_constrained)\n",
    "\n",
    "# Minimize negative log-likelihood via BFGS\n",
    "optim_results_constrained = tfp.optimizer.bfgs_minimize(target_grad_GeneralizedGamma_Constrained, start_param_constrained, tolerance = 1e-10)\n",
    "print(\"\\nOutput of optimizer:\")\n",
    "tf.print(optim_results_constrained)\n",
    "est_params_constrained = optim_results_constrained.position.numpy()\n",
    "\n",
    "d_fitted_constrained = tf.constant(est_params_constrained[0], dtype = tf.float32)\n",
    "p_fitted_constrained = tf.constant(est_params_constrained[1], dtype = tf.float32)\n",
    "a_fitted_constrained = constrained_mean / (tf.math.exp(tf.math.lgamma((d_fitted_constrained+1)/p_fitted_constrained)) / tf.math.exp(tf.math.lgamma(d_fitted_constrained/p_fitted_constrained)))\n",
    "\n",
    "print(\"\\nEstimated parameters:\")\n",
    "tf.print(a_fitted_constrained)\n",
    "tf.print(d_fitted_constrained)\n",
    "tf.print(p_fitted_constrained)\n",
    "\n",
    "print(\"\\nConstraint is satisfied. Mean equals:\")\n",
    "mu_fitted_constrained = a_fitted_constrained * tf.math.exp(tf.math.lgamma((d_fitted_constrained+1)/p_fitted_constrained)) / tf.math.exp(tf.math.lgamma(d_fitted_constrained/p_fitted_constrained))\n",
    "tf.print(mu_fitted_constrained)\n",
    "\n",
    "# Verify second constraint\n",
    "@tf.function\n",
    "def CDF_GeneralizedGamma(x, a, p, d):\n",
    "    # Using the inverse gamma distribution this way yields the same as the CDF for the generalized gamma distribution\n",
    "    return tf.math.igammac( (d/p), ((x/a)**p) )\n",
    "\n",
    "# Density above threshold:\n",
    "print(\"\\nDensity above threshold of \" + str(top_threshold) +\":\")\n",
    "tf.print( CDF_GeneralizedGamma(top_threshold, a_fitted_constrained, p_fitted_constrained, d_fitted_constrained) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 102917601.255001\n",
      "         Iterations: 18\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 25\n",
      "\n",
      "Unconstrained solution:\n",
      "[3.85740727 1.02209275 0.82695621]\n",
      "\n",
      "Unconstrained mean of the fitted distribution:\n",
      "5.429602872289771\n",
      "\n",
      "\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 104750204.50078872\n",
      "            Iterations: 4\n",
      "            Function evaluations: 45\n",
      "            Gradient evaluations: 4\n",
      "\n",
      "Constrained solution:\n",
      "[2.13132661 1.09831352 0.58882419]\n",
      "\n",
      "Constrained mean of the fitted distribution:\n",
      "7.999999698136261\n",
      "\n",
      "Density above threshold of 200.0:\n",
      "5.681601776561191e-06\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in power\n",
      "C:\\Users\\ezequiel\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 105688839.12171128\n",
      "            Iterations: 252\n",
      "            Function evaluations: 1630\n",
      "            Gradient evaluations: 252\n",
      "\n",
      "Constrained solution:\n",
      "[2.16172617e-04 2.10199194e+00 2.27016312e-01]\n",
      "\n",
      "Constrained mean of the fitted distribution:\n",
      "7.999999960693105\n",
      "\n",
      "Density above threshold of 200.0:\n",
      "0.0004999999794301703\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Perform Maximum Likelihood Estimation with constraints using BFGS in Scipy\n",
    "\n",
    "start_param_scipy = test_param.numpy()\n",
    "\n",
    "mydata = [data[:,0], data[:,1], np.sum(data[:,1])]\n",
    "\n",
    "def objective_GeneralizedGamma(params, mydata):\n",
    "    # Retrieve parameters\n",
    "    a = params[0]\n",
    "    d = params[1]\n",
    "    p = params[2]\n",
    "    \n",
    "    # Retrieve observations\n",
    "    Y = mydata[0]\n",
    "    Weights = mydata[1]\n",
    "    n = mydata[2]\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    ll = -(n*np.log((p/a**d)/sp.special.gamma(d/p)) + (d-1)*np.sum(Weights*np.log(Y)) - np.sum(Weights*(Y/a)**p))\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def Mean_GeneralizedGamma(params):\n",
    "    a = params[0]\n",
    "    d = params[1]\n",
    "    p = params[2]\n",
    "    \n",
    "    mean = a * sp.special.gamma((d+1)/p) / sp.special.gamma(d/p)\n",
    "    return mean\n",
    "\n",
    "def constrained_mean_fun(params):\n",
    "    cons_mean = Mean_GeneralizedGamma(params) - constrained_mean\n",
    "    return cons_mean\n",
    "\n",
    "def Right_CDF_GeneralizedGamma(params):\n",
    "    a = params[0]\n",
    "    d = params[1]\n",
    "    p = params[2]\n",
    "    \n",
    "    right_cdf = sp.special.gammaincc(d/p, (top_threshold/a)**p)\n",
    "    return right_cdf\n",
    "\n",
    "def constrained_CDF_fun(params):\n",
    "    cons_CDF = Right_CDF_GeneralizedGamma(params) - constrained_density\n",
    "    return cons_CDF\n",
    "\n",
    "# Firstly, test unconstrained optimization\n",
    "#print( objective_GeneralizedGamma(start_param_scipy, mydata) )\n",
    "myoptions = {'disp' : True, 'maxiter' : 1000}\n",
    "unc_results = sp_opt.minimize(objective_GeneralizedGamma, start_param_scipy, args = mydata, options = myoptions)\n",
    "print(\"\\nUnconstrained solution:\")\n",
    "print(unc_results.x)\n",
    "print(\"\\nUnconstrained mean of the fitted distribution:\")\n",
    "unc_fitted_mean = Mean_GeneralizedGamma(unc_results.x)\n",
    "print(unc_fitted_mean)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Then, perform optimization over the constrained mean\n",
    "con_start_param_scipy = [a_fitted_constrained, d_fitted_constrained, p_fitted_constrained]\n",
    "myconstraints = ({'type': 'eq', 'fun' : constrained_mean_fun})\n",
    "con_results = sp_opt.minimize(objective_GeneralizedGamma, con_start_param_scipy, args = mydata, constraints = myconstraints, options = myoptions)\n",
    "print(\"\\nConstrained solution:\")\n",
    "print(con_results.x)\n",
    "print(\"\\nConstrained mean of the fitted distribution:\")\n",
    "con_fitted_mean = Mean_GeneralizedGamma(con_results.x)\n",
    "print(con_fitted_mean)\n",
    "print(\"\\nDensity above threshold of \" + str(top_threshold) +\":\")\n",
    "print(Right_CDF_GeneralizedGamma(con_results.x))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Finally, perform optimization over the constrained mean and the constrained cdf\n",
    "con_start_param_scipy = [a_fitted_constrained, d_fitted_constrained, p_fitted_constrained]\n",
    "myconstraints = ({'type': 'eq', 'fun' : constrained_mean_fun}, {'type': 'eq', 'fun' : constrained_CDF_fun})\n",
    "con_results = sp_opt.minimize(objective_GeneralizedGamma, con_start_param_scipy, args = mydata, constraints = myconstraints, options = myoptions)\n",
    "print(\"\\nConstrained solution:\")\n",
    "print(con_results.x)\n",
    "print(\"\\nConstrained mean of the fitted distribution:\")\n",
    "con_fitted_mean = Mean_GeneralizedGamma(con_results.x)\n",
    "print(con_fitted_mean)\n",
    "print(\"\\nDensity above threshold of \" + str(top_threshold) +\":\")\n",
    "print(Right_CDF_GeneralizedGamma(con_results.x))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
